{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10fc201d",
   "metadata": {},
   "source": [
    "Day 4: NumPy for ML Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f32c5a5",
   "metadata": {},
   "source": [
    "     1. Data Cleaning with NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be05bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.array([1, 2, np.nan, 4, 5, np.nan, 7])\n",
    "\n",
    "# Find NaN values\n",
    "print(np.isnan(data))\n",
    "\n",
    "# Replace NaN with mean\n",
    "data[np.isnan(data)] = np.nanmean(data)\n",
    "print(\"Cleaned:\", data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c99ae1",
   "metadata": {},
   "source": [
    "    2. Feature Scaling (Normalization & Standardization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bacd84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[20, 20000],\n",
    "              [30, 50000],\n",
    "              [40, 80000],\n",
    "              [50, 100000]])\n",
    "\n",
    "# Z-score standardization\n",
    "mean = X.mean(axis=0)\n",
    "std = X.std(axis=0)\n",
    "\n",
    "X_standardized = (X - mean) / std\n",
    "print(X_standardized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb65321",
   "metadata": {},
   "source": [
    "    3.One-Hot Encoding with NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68ca97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = np.array([\"red\", \"blue\", \"green\", \"red\", \"blue\"])\n",
    "\n",
    "unique, encoded = np.unique(categories, return_inverse=True)\n",
    "one_hot = np.eye(len(unique))[encoded]\n",
    "print(one_hot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ba9c54",
   "metadata": {},
   "source": [
    "    4.Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88372c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.arange(100).reshape(50,2)  # 50 samples, 2 features\n",
    "np.random.shuffle(data)\n",
    "\n",
    "train_size = int(0.8 * len(data))\n",
    "train, test = data[:train_size], data[train_size:]\n",
    "\n",
    "print(\"Train shape:\", train.shape)\n",
    "print(\"Test shape:\", test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29989d1",
   "metadata": {},
   "source": [
    "    5.Mini-batch Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672fb1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.arange(100).reshape(50,2)\n",
    "batch = np.random.choice(len(X), size=8, replace=False)\n",
    "print(\"Mini-batch:\\n\", X[batch])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fd7936",
   "metadata": {},
   "source": [
    "    Phase -4 Exercise:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f5e330",
   "metadata": {},
   "source": [
    "1. Create an array with NaN values and replace them with the column mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2278140",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69cd644c",
   "metadata": {},
   "source": [
    "2. Standardize a dataset of shape (6,3) (random numbers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318c20b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9c6210b",
   "metadata": {},
   "source": [
    "3. Convert the categories [\"apple\",\"banana\",\"apple\",\"orange\"] into one-hot encoded form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90125251",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "077f2de0",
   "metadata": {},
   "source": [
    "4. Shuffle and split a dataset of shape (100,5) into 70% train and 30% test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64cb867",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5f17fdb",
   "metadata": {},
   "source": [
    "5. From a dataset of shape (200,4), randomly select a mini-batch of 16 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e2cd9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71953412",
   "metadata": {},
   "source": [
    "     Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2086bedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. Create an array with NaN values and replace them with column mean\n",
    "data = np.array([[1, 2, np.nan],\n",
    "                 [4, np.nan, 6],\n",
    "                 [7, 8, 9]], dtype=float)\n",
    "\n",
    "col_mean = np.nanmean(data, axis=0)         # mean per column (ignoring NaN)\n",
    "inds = np.where(np.isnan(data))             # find NaN positions\n",
    "data[inds] = np.take(col_mean, inds[1])     # replace with col mean\n",
    "print(\"Array after replacing NaN:\\n\", data)\n",
    "\n",
    "\n",
    "# 2. Standardize a dataset of shape (6,3) (random numbers)\n",
    "dataset = np.random.randint(1, 20, (6, 3)).astype(float)\n",
    "mean = dataset.mean(axis=0)\n",
    "std = dataset.std(axis=0)\n",
    "standardized = (dataset - mean) / std\n",
    "print(\"\\nStandardized dataset:\\n\", standardized)\n",
    "\n",
    "\n",
    "# 3. One-hot encode categories\n",
    "categories = np.array([\"apple\",\"banana\",\"apple\",\"orange\"])\n",
    "unique = np.unique(categories)\n",
    "one_hot = np.zeros((len(categories), len(unique)))\n",
    "\n",
    "for i, val in enumerate(categories):\n",
    "    one_hot[i, unique.tolist().index(val)] = 1\n",
    "\n",
    "print(\"\\nOne-hot encoded:\\n\", one_hot)\n",
    "\n",
    "\n",
    "# 4. Shuffle and split dataset (100,5) â†’ 70% train, 30% test\n",
    "dataset2 = np.random.rand(100, 5)\n",
    "np.random.shuffle(dataset2)\n",
    "\n",
    "split = int(0.7 * len(dataset2))\n",
    "train, test = dataset2[:split], dataset2[split:]\n",
    "print(\"\\nTrain shape:\", train.shape)\n",
    "print(\"Test shape:\", test.shape)\n",
    "\n",
    "\n",
    "# 5. Random mini-batch of 16 samples from dataset (200,4)\n",
    "dataset3 = np.random.rand(200, 4)\n",
    "indices = np.random.choice(200, 16, replace=False)\n",
    "mini_batch = dataset3[indices]\n",
    "print(\"\\nMini-batch (16 samples):\\n\", mini_batch)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
